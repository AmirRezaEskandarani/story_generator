{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8nVSgVOOLeg"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOsdSNeZ-ZOf",
        "outputId": "46986531-eb4d-406b-e36f-2a16ee95ca76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 158888, done.\u001b[K\n",
            "remote: Counting objects: 100% (26844/26844), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1589/1589), done.\u001b[K\n",
            "remote: Total 158888 (delta 26169), reused 25289 (delta 25243), pack-reused 132044\u001b[K\n",
            "Receiving objects: 100% (158888/158888), 157.21 MiB | 27.78 MiB/s, done.\n",
            "Resolving deltas: 100% (119566/119566), done.\n",
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.33.0.dev0)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.33.0.dev0-py3-none-any.whl size=7634320 sha256=e75b66f92ea030582dded4b1f1a41ab7e3a90e2eb93a5b3b7bdb3c342f8afa16\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8xmnhrmi/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcugz62V9_ON"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOACwr-T-ebp",
        "outputId": "c12b0eaa-bcef-4c75-a0b8-bdcdaaa24d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-UdcejOQ1M"
      },
      "source": [
        "## Loading the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM-1Xwjh-J6J"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model from the saved directory\n",
        "model_name = \"/content/drive/MyDrive/story generator/story-gen-model\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1WRPLN0OXGC"
      },
      "source": [
        "## generate story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoniVoLFDJDk"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt,k=0,p=0.9,output_length=300,temperature=1,num_return_sequences=1,repetition_penalty=1.0):\n",
        "    print(\"====prompt====\\n\")\n",
        "    print(prompt+\"\\n\")\n",
        "    print('====target story is as below===\\n')\n",
        "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    # generate story\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=encoded_prompt,\n",
        "        max_length=output_length,\n",
        "        temperature=temperature,\n",
        "        top_k=k,\n",
        "        top_p=p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=num_return_sequences\n",
        "    )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "        # Remove all text after eos token\n",
        "        text = text[: text.find(tokenizer.eos_token)]\n",
        "        print(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# change prompt to generate new stories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebenkGtGEfS5",
        "outputId": "def3ffc5-1d7b-4464-b978-f7ae9d564324"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====prompt====\n",
            "\n",
            "when I was child\n",
            "\n",
            "====target story is as below===\n",
            "\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "when I was child? Surely there must be something bigger hiding in plain sight? No? Well I have, and they are outside of it! \n",
            " \n",
            " Wait, she actually looks about 14 years old. How? It's 5 AM? How are you even dressed? Oh, how tired you are, you're young and ready to go. \n",
            " \n",
            " I ran up to her, all black - just past me, here. Two other kids, one big and not big. No, no, what the fuck is wrong with you! \n",
            " \n",
            " I call her Mike. `` Just the Kid, `` - that's the stupidest name on the block. Her mom runs the gang of dogs. I call her Ben - who only ever shows up at school alone. I go the nicestie around and the principal would definitely say something like that. \n",
            " \n",
            " Mike is so mean, I can't even take her to dinner. What the fuck! I have to get some beef and Sarah must be complaining of how bad she is. That's where all of the last problems are. I decided I would try to fix that one for now, and if she didn't come back when I was old enough, she would be more than happy to return home. So I called her that day - where she came from. A lot of things went wrong with her because it was the whole time that she was supposed to go to school. That didn't see\n"
          ]
        }
      ],
      "source": [
        "prompt = \"when I was child\"\n",
        "generated_text = generate_text(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LSESuum_hsg",
        "outputId": "1e645e7a-3848-4731-aa57-5dfa84546810"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====prompt====\n",
            "\n",
            "Once upon a time in a magical kingdom\n",
            "\n",
            "====target story is as below===\n",
            "\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "Once upon a time in a magical kingdom a dragon comes out with a weapon for the strongest hero. You use it to defeat the tyrant king. Upon his return, a new power has been born. Your prince is unblocked by this new power and grants you a lifetime of terror and with it comes fear! <sep> I rise in the sky like a magic dragon, fortifying my kingdom. My wings and scales hit the ground like a thousand blocks, yet I do not keep up the work, and I am worn and it hurts. My city pokes hard through the forest to one side, and I am awash with deep, black black waves. They are not entirely clear on the terrain, I could see far past the smoke and the puff of burning smoke and light. My city grows taller as I grow taller. In this year I even have an easy night out with the sun at my back. But this year it is different. I smell the sweet smell of licorice. I am not sure if this is something that I want, I am not even sure if I want to sleep or die. The smell of licorice has become difficult to sniff, but this day is different. My daughter and I have had a glass of wine and are going to have dinner Sharma! The aroma of licorice is unsettling. Even my mother, still far too young, tells me that my mother is really disturbed. My mother's scent when I touch her \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "Once upon a time in a magical kingdom, the king appears to give one sincere wish, and if granted, you can live a peaceful and happy life until you give it back. <sep> ***Tuesday, June 18th *** \n",
            " \n",
            " ****Jumping into Black Peasant's Green, I walk into the sky. \n",
            " \n",
            " First sight, everything is moving, everyone was in perfect line to catch my attention. \n",
            " \n",
            " I see in front of me two gigantic objects, one that looks like a very large bin on top of the other, one that looked identical to me, the two that appeared immediately and seemed really much larger and more brilliant, the larger one I was staring at thought in my mind is even more ridiculous, the larger one appeared just like the bin that is on the right... \n",
            " \n",
            " As I grab it out of my body, I have only to watch it in action, the spinning made it move faster than anything could even be described. \n",
            " \n",
            " Grabbing the object, I grab it at the same time, and stop it. \n",
            " \n",
            " As I start slowly pulling the object closer to me, the object finally stopped moving just a step away... \n",
            " \n",
            " I notice I have a feeling of panic after this, the giant object in front of me has almost collapsed into my body, I could see the strange billowing clouds around it were right behind me in time to avoid it coming back....\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "Once upon a time in a magical kingdom it would be normal to see people slowly and silently cross paths with dragons. Well that's about to state. That is, until you discover that no person on Earth has ever actually been on a dragon's tail before. <sep> `` A wormholes? '' No, something may have hooked them up to a certain energy source, you had no idea. Not even a trace of it. Just a mushroom. It looked like a tire or a piece of hair and one coat of makeup. \n",
            " \n",
            " `` No one here has ever been in a wormhole '' I shouted. \n",
            " \n",
            " `` Yes, well, I know it seems like a wormhole but I've never been here before '' the serpent said laughing. \n",
            " \n",
            " *Ring of fire for you, * I shouted. *So that was once. * \n",
            " \n",
            " *That's how it works, * I thought. * \n",
            " \n",
            " He sounded amused. \n",
            " \n",
            " *Because I don't even know where you live. You're kind of nuts * \n",
            " \n",
            " `` That's right, -by the way, you're getting eaten * \n",
            " \n",
            " *What are you, clogg? * \n",
            " \n",
            " `` I didn't ask - I asked no one, and it ain't coming up. '' he replied with a mild smile on his Alcubierre half tongue. \n",
            " \n",
            " *Yes\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a magical kingdom\"\n",
        "generated_text = generate_text(prompt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
